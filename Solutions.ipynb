{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ine-divider](https://user-images.githubusercontent.com/7065401/92672068-398e8080-f2ee-11ea-82d6-ad53f7feb5c0.png)\n",
    "<hr>\n",
    "\n",
    "# Web scraping in Python\n",
    "\n",
    "## Robot exclusion protocol\n",
    "\n",
    "In this project you will perform some web scraping with a sensitivity to robot exclusion rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding multiple contact pages \n",
    "\n",
    "For this task, you should construct a robot to do the following steps:\n",
    "\n",
    "1. Peform a search of a popular search engine, such as Google, Yandex, or Baidu, for a term you pass in.\n",
    "2. Collect all the results for the term (a first page worth of results is fine).\n",
    "3. Extract the domains from the results\n",
    "4. Determine if you are permitted to request the \"Contact\" page for the site.  E.g. `http://example.com/contact`. For the check, determine this permission for at least the following user agents:\n",
    "   * MyRobot\n",
    "   * Fetch\n",
    "   * Microsoft.URL.Control\n",
    "   * Xenu\n",
    "5. If you are permitted to request that resource, determine if it exists at all.\n",
    "6. Create a table of possible answers, with values PROHIBITED, NON-EXISTENT, PERMITTED for each user agent and domain.\n",
    "\n",
    "Such a table might look something like this:\n",
    "\n",
    "```python\n",
    ">>> permissions = term_permissions('robots', 'google.com')\n",
    ">>> pd.DataFrame(permissions).sample(10)\n",
    "```\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Agent</th>\n",
    "      <th>Domain</th>\n",
    "      <th>Status</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>82</th>\n",
    "      <td>MyRobot</td>\n",
    "      <td>spectrum.ieee.org</td>\n",
    "      <td>PERMITTED</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>111</th>\n",
    "      <td>Fetch</td>\n",
    "      <td>m.imdb.com</td>\n",
    "      <td>NON-EXISTENT</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>24</th>\n",
    "      <td>Microsoft.URL.Control</td>\n",
    "      <td>en.wikipedia.org</td>\n",
    "      <td>PROHIBITED</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>92</th>\n",
    "      <td>Microsoft.URL.Control</td>\n",
    "      <td>spectrum.ieee.org</td>\n",
    "      <td>PERMITTED</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>81</th>\n",
    "      <td>Xenu</td>\n",
    "      <td>spectrum.ieee.org</td>\n",
    "      <td>PERMITTED</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>65</th>\n",
    "      <td>Xenu</td>\n",
    "      <td>abcstlouis.com</td>\n",
    "      <td>NON-EXISTENT</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>20</th>\n",
    "      <td>Microsoft.URL.Control</td>\n",
    "      <td>builtin.com</td>\n",
    "      <td>PROHIBITED</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>MyRobot</td>\n",
    "      <td>www.wired.com</td>\n",
    "      <td>PROHIBITED</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>39</th>\n",
    "      <td>Fetch</td>\n",
    "      <td>maps.google.com</td>\n",
    "      <td>PROHIBITED</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>38</th>\n",
    "      <td>MyRobot</td>\n",
    "      <td>maps.google.com</td>\n",
    "      <td>PROHIBITED</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import robotparser\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_link_domains(links):\n",
    "    # Google (and other search engines) force links to go \n",
    "    # through their redirect and tracking keys\n",
    "    domains = set()\n",
    "    for link in links:\n",
    "        if link.startswith('/url?q='):\n",
    "            link = link[7:]\n",
    "        domain = urlparse(link).hostname\n",
    "        if domain:   # Might be None\n",
    "            domains.add(domain)\n",
    "    return domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_permissions(term, \n",
    "                     search_engine=\"google.com\",\n",
    "                     robots=[\"Microsoft.URL.Control\", \n",
    "                             \"Xenu\", \"MyRobot\", \"Fetch\"]):\n",
    "    # For now we have only implemented Google search support\n",
    "    if search_engine != \"google.com\":\n",
    "        raise NotImplementedError(\"Non-Google search engines pending\")\n",
    "    \n",
    "    # Do the initial search on the term\n",
    "    tuples = []\n",
    "    parser = robotparser.RobotFileParser()\n",
    "    query = f\"https://{search_engine}/search?q={term}\"\n",
    "    result = BeautifulSoup(requests.get(query).text)\n",
    "    links = [a['href'] for a in result.find_all('a')]\n",
    "    domains = google_link_domains(links)\n",
    "    \n",
    "    # Figure out which contact page requests are prohibited\n",
    "    to_contact = []\n",
    "    for domain in domains:\n",
    "        parser.set_url(f\"https://{domain}/robots.txt\")\n",
    "        parser.read()\n",
    "        for robot in robots:\n",
    "            contact = f\"https://{domain}/contact\"\n",
    "            if parser.can_fetch(robot, contact):\n",
    "                to_contact.append(contact)\n",
    "            else:\n",
    "                tuples.append((robot, domain, 'PROHIBITED'))\n",
    "    \n",
    "    # Of the permitted ones, determine if the page actually exists\n",
    "    # The server MIGHT give different status per user agent\n",
    "    for contact in to_contact:\n",
    "        domain = urlparse(contact).hostname\n",
    "        for robot in robots:\n",
    "            header = {'User-agent': robot}\n",
    "            try:\n",
    "                # Might take too long to get the contact page\n",
    "                contact_page = requests.get(contact, \n",
    "                                            timeout=0.1, \n",
    "                                            headers=header)\n",
    "            except (requests.exceptions.Timeout, Exception):\n",
    "                # Not obvious the cause, but let's call it 503 status\n",
    "                contact_page.status_code = 503\n",
    "                \n",
    "            if contact_page.status_code == 200:\n",
    "                tuples.append((robot, domain, \"PERMITTED\"))\n",
    "            else:\n",
    "                tuples.append((robot, domain, \"NON-EXISTENT\"))\n",
    "    \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Agent</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microsoft.URL.Control</td>\n",
       "      <td>en.wikipedia.org</td>\n",
       "      <td>PROHIBITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenu</td>\n",
       "      <td>en.wikipedia.org</td>\n",
       "      <td>PROHIBITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fetch</td>\n",
       "      <td>en.wikipedia.org</td>\n",
       "      <td>PROHIBITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microsoft.URL.Control</td>\n",
       "      <td>www.livescience.com</td>\n",
       "      <td>PROHIBITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xenu</td>\n",
       "      <td>www.livescience.com</td>\n",
       "      <td>PROHIBITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Fetch</td>\n",
       "      <td>www.nationalgeographic.com</td>\n",
       "      <td>NON-EXISTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Microsoft.URL.Control</td>\n",
       "      <td>www.nationalgeographic.com</td>\n",
       "      <td>NON-EXISTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Xenu</td>\n",
       "      <td>www.nationalgeographic.com</td>\n",
       "      <td>NON-EXISTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>MyRobot</td>\n",
       "      <td>www.nationalgeographic.com</td>\n",
       "      <td>NON-EXISTENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Fetch</td>\n",
       "      <td>www.nationalgeographic.com</td>\n",
       "      <td>NON-EXISTENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Agent                      Domain        Status\n",
       "0    Microsoft.URL.Control            en.wikipedia.org    PROHIBITED\n",
       "1                     Xenu            en.wikipedia.org    PROHIBITED\n",
       "2                    Fetch            en.wikipedia.org    PROHIBITED\n",
       "3    Microsoft.URL.Control         www.livescience.com    PROHIBITED\n",
       "4                     Xenu         www.livescience.com    PROHIBITED\n",
       "..                     ...                         ...           ...\n",
       "142                  Fetch  www.nationalgeographic.com  NON-EXISTENT\n",
       "143  Microsoft.URL.Control  www.nationalgeographic.com  NON-EXISTENT\n",
       "144                   Xenu  www.nationalgeographic.com  NON-EXISTENT\n",
       "145                MyRobot  www.nationalgeographic.com  NON-EXISTENT\n",
       "146                  Fetch  www.nationalgeographic.com  NON-EXISTENT\n",
       "\n",
       "[147 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permissions = term_permissions('spiders')\n",
    "pd.DataFrame(permissions, columns=['Agent', 'Domain', 'Status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
